{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SL NO</th>\n",
       "      <th>From Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>Nox</th>\n",
       "      <th>NH3</th>\n",
       "      <th>SO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>Ozone</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Eth-Benzene</th>\n",
       "      <th>MP-Xylene</th>\n",
       "      <th>WS</th>\n",
       "      <th>WD</th>\n",
       "      <th>SR</th>\n",
       "      <th>BP</th>\n",
       "      <th>AT</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18-02-2019 00:00</td>\n",
       "      <td>58.04</td>\n",
       "      <td>81.06</td>\n",
       "      <td>1.81</td>\n",
       "      <td>6.40</td>\n",
       "      <td>9.66</td>\n",
       "      <td>27.18</td>\n",
       "      <td>13.77</td>\n",
       "      <td>0.49</td>\n",
       "      <td>15.73</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.86</td>\n",
       "      <td>229.44</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1002.19</td>\n",
       "      <td>20.87</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18-02-2019 01:00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>82.39</td>\n",
       "      <td>1.91</td>\n",
       "      <td>6.08</td>\n",
       "      <td>9.70</td>\n",
       "      <td>26.46</td>\n",
       "      <td>13.64</td>\n",
       "      <td>0.44</td>\n",
       "      <td>18.36</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.54</td>\n",
       "      <td>217.20</td>\n",
       "      <td>1.46</td>\n",
       "      <td>956.00</td>\n",
       "      <td>18.92</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>18-02-2019 02:00</td>\n",
       "      <td>55.50</td>\n",
       "      <td>73.42</td>\n",
       "      <td>2.22</td>\n",
       "      <td>6.71</td>\n",
       "      <td>10.86</td>\n",
       "      <td>27.06</td>\n",
       "      <td>13.64</td>\n",
       "      <td>0.44</td>\n",
       "      <td>34.54</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.66</td>\n",
       "      <td>197.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001.60</td>\n",
       "      <td>18.33</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>18-02-2019 03:00</td>\n",
       "      <td>66.56</td>\n",
       "      <td>96.04</td>\n",
       "      <td>2.79</td>\n",
       "      <td>7.00</td>\n",
       "      <td>12.10</td>\n",
       "      <td>26.34</td>\n",
       "      <td>13.72</td>\n",
       "      <td>0.45</td>\n",
       "      <td>31.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.61</td>\n",
       "      <td>206.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001.41</td>\n",
       "      <td>17.63</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>18-02-2019 04:00</td>\n",
       "      <td>67.08</td>\n",
       "      <td>97.21</td>\n",
       "      <td>3.29</td>\n",
       "      <td>9.45</td>\n",
       "      <td>15.53</td>\n",
       "      <td>25.16</td>\n",
       "      <td>13.80</td>\n",
       "      <td>0.47</td>\n",
       "      <td>24.07</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.54</td>\n",
       "      <td>230.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>980.67</td>\n",
       "      <td>16.61</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33062</th>\n",
       "      <td>33063</td>\n",
       "      <td>30-12-2022 20:00</td>\n",
       "      <td>295.72</td>\n",
       "      <td>371.83</td>\n",
       "      <td>5.54</td>\n",
       "      <td>2.62</td>\n",
       "      <td>5.41</td>\n",
       "      <td>16.13</td>\n",
       "      <td>13.91</td>\n",
       "      <td>2.64</td>\n",
       "      <td>22.82</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.16</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.31</td>\n",
       "      <td>168.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1009.22</td>\n",
       "      <td>15.96</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33063</th>\n",
       "      <td>33064</td>\n",
       "      <td>30-12-2022 21:00</td>\n",
       "      <td>362.60</td>\n",
       "      <td>448.50</td>\n",
       "      <td>5.61</td>\n",
       "      <td>2.62</td>\n",
       "      <td>5.46</td>\n",
       "      <td>17.45</td>\n",
       "      <td>13.88</td>\n",
       "      <td>2.79</td>\n",
       "      <td>5.66</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.69</td>\n",
       "      <td>132.27</td>\n",
       "      <td>11.17</td>\n",
       "      <td>967.18</td>\n",
       "      <td>14.93</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33064</th>\n",
       "      <td>33065</td>\n",
       "      <td>30-12-2022 22:00</td>\n",
       "      <td>277.86</td>\n",
       "      <td>353.78</td>\n",
       "      <td>5.52</td>\n",
       "      <td>2.46</td>\n",
       "      <td>5.29</td>\n",
       "      <td>17.92</td>\n",
       "      <td>13.74</td>\n",
       "      <td>3.25</td>\n",
       "      <td>22.64</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.89</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.36</td>\n",
       "      <td>173.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1009.11</td>\n",
       "      <td>15.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33065</th>\n",
       "      <td>33066</td>\n",
       "      <td>30-12-2022 23:00</td>\n",
       "      <td>380.00</td>\n",
       "      <td>450.48</td>\n",
       "      <td>5.46</td>\n",
       "      <td>2.47</td>\n",
       "      <td>5.32</td>\n",
       "      <td>18.01</td>\n",
       "      <td>14.12</td>\n",
       "      <td>3.28</td>\n",
       "      <td>7.07</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.94</td>\n",
       "      <td>4.21</td>\n",
       "      <td>0.82</td>\n",
       "      <td>163.19</td>\n",
       "      <td>9.57</td>\n",
       "      <td>912.74</td>\n",
       "      <td>13.45</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33066</th>\n",
       "      <td>33067</td>\n",
       "      <td>31-12-2022 00:00</td>\n",
       "      <td>295.00</td>\n",
       "      <td>389.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.27</td>\n",
       "      <td>0.43</td>\n",
       "      <td>109.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33067 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SL NO         From Date   PM2.5    PM10    NO   NO2    Nox    NH3  \\\n",
       "0          1  18-02-2019 00:00   58.04   81.06  1.81  6.40   9.66  27.18   \n",
       "1          2  18-02-2019 01:00   60.00   82.39  1.91  6.08   9.70  26.46   \n",
       "2          3  18-02-2019 02:00   55.50   73.42  2.22  6.71  10.86  27.06   \n",
       "3          4  18-02-2019 03:00   66.56   96.04  2.79  7.00  12.10  26.34   \n",
       "4          5  18-02-2019 04:00   67.08   97.21  3.29  9.45  15.53  25.16   \n",
       "...      ...               ...     ...     ...   ...   ...    ...    ...   \n",
       "33062  33063  30-12-2022 20:00  295.72  371.83  5.54  2.62   5.41  16.13   \n",
       "33063  33064  30-12-2022 21:00  362.60  448.50  5.61  2.62   5.46  17.45   \n",
       "33064  33065  30-12-2022 22:00  277.86  353.78  5.52  2.46   5.29  17.92   \n",
       "33065  33066  30-12-2022 23:00  380.00  450.48  5.46  2.47   5.32  18.01   \n",
       "33066  33067  31-12-2022 00:00  295.00  389.00   NaN   NaN    NaN    NaN   \n",
       "\n",
       "         SO2    CO  Ozone  Benzene  Eth-Benzene  MP-Xylene    WS      WD  \\\n",
       "0      13.77  0.49  15.73     0.92          NaN       0.33  1.86  229.44   \n",
       "1      13.64  0.44  18.36     0.84          NaN       0.29  1.54  217.20   \n",
       "2      13.64  0.44  34.54     0.77          NaN       0.23  0.66  197.21   \n",
       "3      13.72  0.45  31.85     0.82          NaN       0.21  0.61  206.44   \n",
       "4      13.80  0.47  24.07     0.74          NaN       0.22  0.54  230.53   \n",
       "...      ...   ...    ...      ...          ...        ...   ...     ...   \n",
       "33062  13.91  2.64  22.82     2.96         3.16       3.37  0.31  168.12   \n",
       "33063  13.88  2.79   5.66     3.12         3.34       3.56  0.69  132.27   \n",
       "33064  13.74  3.25  22.64     3.64         3.89       4.15  0.36  173.64   \n",
       "33065  14.12  3.28   7.07     3.68         3.94       4.21  0.82  163.19   \n",
       "33066    NaN   NaN    NaN      NaN         4.00       4.27  0.43  109.33   \n",
       "\n",
       "          SR       BP     AT    RF  \n",
       "0       0.22  1002.19  20.87  0.00  \n",
       "1       1.46   956.00  18.92  0.24  \n",
       "2        NaN  1001.60  18.33  0.00  \n",
       "3        NaN  1001.41  17.63  0.00  \n",
       "4        NaN   980.67  16.61  0.00  \n",
       "...      ...      ...    ...   ...  \n",
       "33062    NaN  1009.22  15.96  0.00  \n",
       "33063  11.17   967.18  14.93  0.48  \n",
       "33064    NaN  1009.11  15.22  0.00  \n",
       "33065   9.57   912.74  13.45  1.09  \n",
       "33066    NaN      NaN  14.60  0.00  \n",
       "\n",
       "[33067 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  dataset loading\n",
    "data = pd.read_csv(r\"C:\\Users\\himan\\Desktop\\7th GAN\\updataDataSets.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns like 'SL NO' and 'From Date'\n",
    "data = data.drop(columns=['SL NO', 'From Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaceing NaN values with 0\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dataset has columns for features and a target variable\n",
    "\n",
    "feature_columns = ['PM10', 'NO', 'NO2', 'Nox', 'NH3', 'SO2', 'CO', 'Ozone', 'Benzene', 'Eth-Benzene', 'MP-Xylene', 'WS', 'WD', 'SR', 'BP', 'AT', 'RF']\n",
    "target_column = 'PM2.5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features and target variable\n",
    "X = data[feature_columns].values\n",
    "y = data[target_column].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the feature values\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sliding window parameters\n",
    "window_size = 5  # Number of past time steps considered\n",
    "output_size = 10  # Number of steps ahead used to predict pollutant concentration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D dataset using sliding window technique\n",
    "def create_dataset(X, y, window_size, output_size):\n",
    "    X_windowed = []\n",
    "    y_windowed = []\n",
    "    for i in range(len(X) - window_size - output_size + 1):\n",
    "        X_windowed.append(X[i:i+window_size])\n",
    "        y_windowed.append(y[i+window_size:i+window_size+output_size])\n",
    "    return np.array(X_windowed), np.array(y_windowed)\n",
    "\n",
    "X_windowed, y_windowed = create_dataset(X_scaled, y, window_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_windowed, y_windowed, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Bidirectional(LSTM(units=50)))\n",
    "model.add(Dense(output_size))  # Output layer with 'output_size' neurons for multi-step ahead prediction\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "827/827 [==============================] - 2468s 3s/step - loss: 4891.5986 - val_loss: 3870.8667\n",
      "Epoch 2/100\n",
      "827/827 [==============================] - 13s 15ms/step - loss: 3282.0125 - val_loss: 2493.7092\n",
      "Epoch 3/100\n",
      "827/827 [==============================] - 9s 11ms/step - loss: 2545.7942 - val_loss: 2179.9624\n",
      "Epoch 4/100\n",
      "827/827 [==============================] - 7s 8ms/step - loss: 2369.8171 - val_loss: 2110.3740\n",
      "Epoch 5/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 2217.2007 - val_loss: 1929.9298\n",
      "Epoch 6/100\n",
      "827/827 [==============================] - 7s 8ms/step - loss: 2092.2839 - val_loss: 1881.8032\n",
      "Epoch 7/100\n",
      "827/827 [==============================] - 7s 8ms/step - loss: 1993.5674 - val_loss: 1727.2281\n",
      "Epoch 8/100\n",
      "827/827 [==============================] - 7s 8ms/step - loss: 1904.3510 - val_loss: 1691.4097\n",
      "Epoch 9/100\n",
      "827/827 [==============================] - 7s 8ms/step - loss: 1835.9005 - val_loss: 1609.1215\n",
      "Epoch 10/100\n",
      "827/827 [==============================] - 8s 9ms/step - loss: 1779.5048 - val_loss: 1543.9420\n",
      "Epoch 11/100\n",
      "827/827 [==============================] - 7s 8ms/step - loss: 1727.0083 - val_loss: 1512.8350\n",
      "Epoch 12/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1681.7599 - val_loss: 1490.8993\n",
      "Epoch 13/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1646.2329 - val_loss: 1485.4213\n",
      "Epoch 14/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1622.0031 - val_loss: 1464.8854\n",
      "Epoch 15/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1596.2732 - val_loss: 1438.6042\n",
      "Epoch 16/100\n",
      "827/827 [==============================] - 9s 10ms/step - loss: 1575.4456 - val_loss: 1464.2959\n",
      "Epoch 17/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1552.3252 - val_loss: 1401.2777\n",
      "Epoch 18/100\n",
      "827/827 [==============================] - 9s 11ms/step - loss: 1528.4962 - val_loss: 1413.8781\n",
      "Epoch 19/100\n",
      "827/827 [==============================] - 8s 10ms/step - loss: 1518.6929 - val_loss: 1377.3605\n",
      "Epoch 20/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1498.8866 - val_loss: 1360.7666\n",
      "Epoch 21/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1478.2686 - val_loss: 1358.9974\n",
      "Epoch 22/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1467.4515 - val_loss: 1344.4210\n",
      "Epoch 23/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1455.8862 - val_loss: 1349.9821\n",
      "Epoch 24/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1432.5028 - val_loss: 1327.4744\n",
      "Epoch 25/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1421.4663 - val_loss: 1331.0037\n",
      "Epoch 26/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1413.9135 - val_loss: 1325.5743\n",
      "Epoch 27/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1403.5887 - val_loss: 1446.8956\n",
      "Epoch 28/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1397.7927 - val_loss: 1316.2069\n",
      "Epoch 29/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1381.8673 - val_loss: 1348.7312\n",
      "Epoch 30/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1361.1498 - val_loss: 1308.3037\n",
      "Epoch 31/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1353.0653 - val_loss: 1311.2693\n",
      "Epoch 32/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1343.5259 - val_loss: 1299.3798\n",
      "Epoch 33/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1336.7988 - val_loss: 1298.4147\n",
      "Epoch 34/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1324.0095 - val_loss: 1284.1965\n",
      "Epoch 35/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1317.5316 - val_loss: 1338.9215\n",
      "Epoch 36/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1299.7823 - val_loss: 1277.9005\n",
      "Epoch 37/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1296.1460 - val_loss: 1282.3629\n",
      "Epoch 38/100\n",
      "827/827 [==============================] - 8s 9ms/step - loss: 1279.0579 - val_loss: 1274.5156\n",
      "Epoch 39/100\n",
      "827/827 [==============================] - 8s 9ms/step - loss: 1269.9938 - val_loss: 1267.2395\n",
      "Epoch 40/100\n",
      "827/827 [==============================] - 8s 10ms/step - loss: 1266.9913 - val_loss: 1289.5800\n",
      "Epoch 41/100\n",
      "827/827 [==============================] - 8s 10ms/step - loss: 1255.1591 - val_loss: 1259.2051\n",
      "Epoch 42/100\n",
      "827/827 [==============================] - 8s 9ms/step - loss: 1242.0675 - val_loss: 1261.5469\n",
      "Epoch 43/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1231.9203 - val_loss: 1297.4662\n",
      "Epoch 44/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1235.0654 - val_loss: 1270.1437\n",
      "Epoch 45/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1218.5178 - val_loss: 1252.1520\n",
      "Epoch 46/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1202.9982 - val_loss: 1245.3116\n",
      "Epoch 47/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1193.5283 - val_loss: 1244.5519\n",
      "Epoch 48/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1181.5890 - val_loss: 1273.4529\n",
      "Epoch 49/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1186.8572 - val_loss: 1258.6156\n",
      "Epoch 50/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1172.5360 - val_loss: 1262.4387\n",
      "Epoch 51/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1164.6744 - val_loss: 1254.3281\n",
      "Epoch 52/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1145.2886 - val_loss: 1239.8916\n",
      "Epoch 53/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1148.9268 - val_loss: 1250.7111\n",
      "Epoch 54/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1126.8671 - val_loss: 1240.3768\n",
      "Epoch 55/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1120.9351 - val_loss: 1249.0586\n",
      "Epoch 56/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1113.6798 - val_loss: 1243.7317\n",
      "Epoch 57/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1144.6926 - val_loss: 1250.2657\n",
      "Epoch 58/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1105.0540 - val_loss: 1252.4889\n",
      "Epoch 59/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1086.4177 - val_loss: 1215.5939\n",
      "Epoch 60/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1088.4521 - val_loss: 1220.0344\n",
      "Epoch 61/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1064.9346 - val_loss: 1245.4126\n",
      "Epoch 62/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1060.5710 - val_loss: 1264.9355\n",
      "Epoch 63/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1059.8326 - val_loss: 1239.7086\n",
      "Epoch 64/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1054.5509 - val_loss: 1273.8035\n",
      "Epoch 65/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1029.9001 - val_loss: 1234.6245\n",
      "Epoch 66/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1032.8892 - val_loss: 1218.1556\n",
      "Epoch 67/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1020.9854 - val_loss: 1225.4453\n",
      "Epoch 68/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1012.4645 - val_loss: 1244.9756\n",
      "Epoch 69/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1019.7625 - val_loss: 1245.5896\n",
      "Epoch 70/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 1005.8713 - val_loss: 1250.7629\n",
      "Epoch 71/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 989.6399 - val_loss: 1242.6056\n",
      "Epoch 72/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 985.0421 - val_loss: 1260.1416\n",
      "Epoch 73/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 987.6382 - val_loss: 1242.9949\n",
      "Epoch 74/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 976.9405 - val_loss: 1222.0522\n",
      "Epoch 75/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 958.8630 - val_loss: 1228.3000\n",
      "Epoch 76/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 953.7168 - val_loss: 1231.4180\n",
      "Epoch 77/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 949.5843 - val_loss: 1236.4089\n",
      "Epoch 78/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 953.8116 - val_loss: 1238.4634\n",
      "Epoch 79/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 940.4535 - val_loss: 1221.9077\n",
      "Epoch 80/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 928.6778 - val_loss: 1228.3054\n",
      "Epoch 81/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 932.5980 - val_loss: 1215.7126\n",
      "Epoch 82/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 910.3831 - val_loss: 1233.6941\n",
      "Epoch 83/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 907.0427 - val_loss: 1228.0790\n",
      "Epoch 84/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 899.4243 - val_loss: 1222.0658\n",
      "Epoch 85/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 914.2456 - val_loss: 1255.2177\n",
      "Epoch 86/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 890.2086 - val_loss: 1231.7484\n",
      "Epoch 87/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 888.6484 - val_loss: 1240.1389\n",
      "Epoch 88/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 893.7089 - val_loss: 1232.8594\n",
      "Epoch 89/100\n",
      "827/827 [==============================] - 7s 9ms/step - loss: 863.9652 - val_loss: 1247.2743\n",
      "Epoch 90/100\n",
      "827/827 [==============================] - 9s 11ms/step - loss: 869.8961 - val_loss: 1233.1855\n",
      "Epoch 91/100\n",
      "827/827 [==============================] - 9s 11ms/step - loss: 858.7198 - val_loss: 1406.7417\n",
      "Epoch 92/100\n",
      "827/827 [==============================] - 9s 11ms/step - loss: 884.8923 - val_loss: 1215.1124\n",
      "Epoch 93/100\n",
      "827/827 [==============================] - 9s 11ms/step - loss: 884.3877 - val_loss: 1211.5493\n",
      "Epoch 94/100\n",
      "827/827 [==============================] - 8s 10ms/step - loss: 844.3690 - val_loss: 1228.9855\n",
      "Epoch 95/100\n",
      "827/827 [==============================] - 8s 10ms/step - loss: 847.7228 - val_loss: 1241.7816\n",
      "Epoch 96/100\n",
      "827/827 [==============================] - 8s 10ms/step - loss: 845.2696 - val_loss: 1225.4313\n",
      "Epoch 97/100\n",
      "827/827 [==============================] - 8s 10ms/step - loss: 832.8365 - val_loss: 1209.5636\n",
      "Epoch 98/100\n",
      "827/827 [==============================] - 8s 10ms/step - loss: 829.6365 - val_loss: 1222.5962\n",
      "Epoch 99/100\n",
      "827/827 [==============================] - 8s 9ms/step - loss: 813.8293 - val_loss: 1247.3071\n",
      "Epoch 100/100\n",
      "827/827 [==============================] - 8s 9ms/step - loss: 836.4640 - val_loss: 1264.8422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eae4da2790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 1s 4ms/step - loss: 1264.8422\n",
      "Test Loss: 1264.8421630859375\n",
      "207/207 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of predictions with a factor of 2 (FAC2): 0.9374073513840568\n",
      "Root Mean Square Error (RMSE): 35.56462288138887\n",
      "Correlation Coefficient (r): 0.8244117061979126\n",
      "Mean Gross Error (MGE): 18.787749665850537\n",
      "Mean Bias (MB): -0.274143013053935\n",
      "Coefficient of Efficiency (COE): 0.6748135861095224\n",
      "Coefficient of Determination (R2 score): 0.6742770039185186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate Fraction of predictions with a factor of 2 (FAC2)\n",
    "def fac2(y_true, y_pred):\n",
    "    within_factor = np.abs(y_true / y_pred) <= 2\n",
    "    return np.mean(within_factor)\n",
    "\n",
    "# Calculate Root Mean Square Error (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "# Calculate Correlation Coefficient (r)\n",
    "r = np.corrcoef(np.ravel(y_test), np.ravel(predictions))[0, 1]\n",
    "\n",
    "# Calculate Mean Gross Error (MGE)\n",
    "mge = np.mean(np.abs(y_test - predictions))\n",
    "\n",
    "# Calculate Mean Bias (MB)\n",
    "mb = np.mean(y_test - predictions)\n",
    "\n",
    "# Calculate Coefficient of Efficiency (COE)\n",
    "coe = 1 - (np.sum((y_test - predictions) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2))\n",
    "\n",
    "# Calculate Coefficient of Determination (R2 score)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Fraction of predictions with a factor of 2 (FAC2): {fac2(y_test, predictions)}')\n",
    "print(f'Root Mean Square Error (RMSE): {rmse}')\n",
    "print(f'Correlation Coefficient (r): {r}')\n",
    "print(f'Mean Gross Error (MGE): {mge}')\n",
    "print(f'Mean Bias (MB): {mb}')\n",
    "print(f'Coefficient of Efficiency (COE): {coe}')\n",
    "print(f'Coefficient of Determination (R2 score): {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
